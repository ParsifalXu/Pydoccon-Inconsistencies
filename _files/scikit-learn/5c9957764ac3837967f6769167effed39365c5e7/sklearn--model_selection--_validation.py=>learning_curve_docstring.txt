Learning curve.

Determines cross-validated training and test scores for different training
set sizes.

A cross-validation generator splits the whole dataset k times in training
and test data. Subsets of the training set with varying sizes will be used
to train the estimator and a score for each training subset size and the
test set will be computed. Afterwards, the scores will be averaged over
all k runs for each training subset size.

Read more in the :ref:`User Guide <learning_curve>`.

Parameters
----------
estimator : object type that implements the "fit" and "predict" methods
    An object of that type which is cloned for each validation.

X : array-like of shape (n_samples, n_features)
    Training vector, where n_samples is the number of samples and
    n_features is the number of features.

y : array-like of shape (n_samples,) or (n_samples, n_outputs)
    Target relative to X for classification or regression;
    None for unsupervised learning.

groups : array-like of  shape (n_samples,), default=None
    Group labels for the samples used while splitting the dataset into
    train/test set. Only used in conjunction with a "Group" :term:`cv`
    instance (e.g., :class:`GroupKFold`).

train_sizes : array-like of shape (n_ticks,),             default=np.linspace(0.1, 1.0, 5)
    Relative or absolute numbers of training examples that will be used to
    generate the learning curve. If the dtype is float, it is regarded as a
    fraction of the maximum size of the training set (that is determined
    by the selected validation method), i.e. it has to be within (0, 1].
    Otherwise it is interpreted as absolute sizes of the training sets.
    Note that for classification the number of samples usually have to
    be big enough to contain at least one sample from each class.

cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:

    - None, to use the default 5-fold cross validation,
    - int, to specify the number of folds in a `(Stratified)KFold`,
    - :term:`CV splitter`,
    - An iterable yielding (train, test) splits as arrays of indices.

    For int/None inputs, if the estimator is a classifier and ``y`` is
    either binary or multiclass, :class:`StratifiedKFold` is used. In all
    other cases, :class:`KFold` is used.

    Refer :ref:`User Guide <cross_validation>` for the various
    cross-validation strategies that can be used here.

    .. versionchanged:: 0.22
        ``cv`` default value if None changed from 3-fold to 5-fold.

scoring : str or callable, default=None
    A str (see model evaluation documentation) or
    a scorer callable object / function with signature
    ``scorer(estimator, X, y)``.

exploit_incremental_learning : bool, default=False
    If the estimator supports incremental learning, this will be
    used to speed up fitting for different training set sizes.

n_jobs : int, default=None
    Number of jobs to run in parallel.
    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
    for more details.

pre_dispatch : int or str, default='all'
    Number of predispatched jobs for parallel execution (default is
    all). The option can reduce the allocated memory. The str can
    be an expression like '2*n_jobs'.

verbose : int, default=0
    Controls the verbosity: the higher, the more messages.

shuffle : bool, default=False
    Whether to shuffle training data before taking prefixes of it
    based on``train_sizes``.

random_state : int or RandomState instance, default=None
    Used when ``shuffle`` is True. Pass an int for reproducible
    output across multiple function calls.
    See :term:`Glossary <random_state>`.

error_score : 'raise' or numeric, default=np.nan
    Value to assign to the score if an error occurs in estimator fitting.
    If set to 'raise', the error is raised.
    If a numeric value is given, FitFailedWarning is raised. This parameter
    does not affect the refit step, which will always raise the error.

return_times : bool, default=False
    Whether to return the fit and score times.

Returns
-------
train_sizes_abs : array of shape (n_unique_ticks,)
    Numbers of training examples that has been used to generate the
    learning curve. Note that the number of ticks might be less
    than n_ticks because duplicate entries will be removed.

train_scores : array of shape (n_ticks, n_cv_folds)
    Scores on training sets.

test_scores : array of shape (n_ticks, n_cv_folds)
    Scores on test set.

fit_times : array of shape (n_ticks, n_cv_folds)
    Times spent for fitting in seconds. Only present if ``return_times``
    is True.

score_times : array of shape (n_ticks, n_cv_folds)
    Times spent for scoring in seconds. Only present if ``return_times``
    is True.

Notes
-----
See :ref:`examples/model_selection/plot_learning_curve.py
<sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`